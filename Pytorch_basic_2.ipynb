{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nuclear-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "#import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch,gzip,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fossil-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "employed-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "planned-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DealDataset(Dataset):\n",
    "    \"\"\"\n",
    "        读取数据、初始化数据\n",
    "    \"\"\"\n",
    "    def __init__(self, folder, data_name, label_name,transform=None):\n",
    "        (train_set, train_labels) = load_data(folder, data_name, label_name) # 其实也可以直接使用torch.load(),读取之后的结果为torch.Tensor形式\n",
    "        self.train_set = train_set\n",
    "        self.train_labels = train_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img, target = self.train_set[index], int(self.train_labels[index])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fabulous-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_folder, data_name, label_name):\n",
    "    \"\"\"\n",
    "        data_folder: 文件目录\n",
    "        data_name： 数据文件名\n",
    "        label_name：标签数据文件名\n",
    "    \"\"\"\n",
    "    with gzip.open(os.path.join(data_folder,label_name), 'rb') as lbpath: # rb表示的是读取二进制数据\n",
    "        y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(os.path.join(data_folder,data_name), 'rb') as imgpath:\n",
    "        x_train = np.frombuffer(\n",
    "            imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28)\n",
    "    return (x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "offensive-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DealDataset('E:\\\\mnist\\\\', \"train-images-idx3-ubyte.gz\",\"train-labels-idx1-ubyte.gz\",transform=transforms.ToTensor())\n",
    "test_dataset = DealDataset('E:\\\\mnist\\\\', \"t10k-images-idx3-ubyte.gz\",\"t10k-labels-idx1-ubyte.gz\",transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expanded-society",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "played-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        # super函数用来解决多重继承时父类的查找问题\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "phantom-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "absolute-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aboriginal-netscape",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\OpenCv\\lib\\site-packages\\torchvision\\transforms\\functional.py:126: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:189.)\n",
      "  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3203\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1984\n",
      "Epoch [1/5], Step [300/600], Loss: 0.3221\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1702\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1167\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0587\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1382\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0391\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0782\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0726\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0302\n",
      "Epoch [2/5], Step [600/600], Loss: 0.1660\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0424\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0732\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0881\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0936\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0275\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0881\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0380\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0667\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0465\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0153\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0405\n",
      "Epoch [4/5], Step [600/600], Loss: 0.1150\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0223\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0450\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0154\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0279\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0880\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0347\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acquired-occupation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.5 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "median-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lovely-press",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 0.9400\n",
      "Epoch [10/60], Loss: 0.6308\n",
      "Epoch [15/60], Loss: 0.5052\n",
      "Epoch [20/60], Loss: 0.4539\n",
      "Epoch [25/60], Loss: 0.4327\n",
      "Epoch [30/60], Loss: 0.4238\n",
      "Epoch [35/60], Loss: 0.4198\n",
      "Epoch [40/60], Loss: 0.4178\n",
      "Epoch [45/60], Loss: 0.4166\n",
      "Epoch [50/60], Loss: 0.4157\n",
      "Epoch [55/60], Loss: 0.4150\n",
      "Epoch [60/60], Loss: 0.4143\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "# Linear regression model\n",
    "# 全连接层\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "# 损失函数和激活函数\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# 训练模型\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:OpenCv] *",
   "language": "python",
   "name": "conda-env-OpenCv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
